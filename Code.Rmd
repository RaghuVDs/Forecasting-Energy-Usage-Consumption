---
title: "IDS_P1_Data_Merge"
output: html_document
date: "2023-12-06"
---

```{r}
#Load the required libraries
library(arrow) # to read the parquet files
library(dplyr) # to merge data files by rows
library(tidyverse) #to read CSV files
library(caret)
```


####################################### Data Merging ###########################################


```{r}
# Read the static house data
static_house_info = read_parquet("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet")

# Create list of all house IDs
house_id <- static_house_info$bldg_id

```

```{r}
# Initialize a storage structure for the full data frame
full_df <- list()
n <- 0

for (i in house_id) {
  energy_url <- paste0("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/2023-houseData/", i, ".parquet")
  
  # Try to read in energy data for the specific house ID
  tryCatch({
    energy <- read_parquet(energy_url)
    energy <- energy[format(energy$time, "%Y-%m") == "2018-07", ] #only keep july dates
 
    energy$total_energy_consumed <- rowSums(energy[, 1:42], na.rm = TRUE)
    energy$bldg_id <- i
    
    # Merge with static house information
    energy_house_merge <- left_join(energy, static_house_info, by = 'bldg_id')
    
    # Merge with weather data
    county <- energy_house_merge$in.county[1]
    weather_url <- paste0("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/weather/2023-weather-data/", county, ".csv")
    weather <- read_csv(weather_url)
    weather$in.county <- county
    energy_house_weather_merge <- energy_house_merge %>% left_join(weather, by = c('in.county', 'time' = 'date_time'))
    
    # Append to the full data frame list
    full_df[[length(full_df) + 1]] <- energy_house_weather_merge
  }, error = function(e) {
    message(paste("Error processing house ID", i, ":", e$message))
  })
  
  # Increment counter and check progress
  n <- n + 1
  print(paste("Processed house ID", i, ":", n, "of", length(house_id)))
}
```

```{r}
# Combine all data frames into one
data_cleaned <- bind_rows(full_df)
```

```{r}
# Save the data frame into a parquet file
#write_parquet(data_cleaned, "/Users/mj/Desktop/IDS_Nikitha/data_merged.parquet")
```


####################################### Data Summarization by day ###########################################

```{r}

library(dplyr)
data_merged= read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_merged.parquet")

# Assuming 'data_cleaned' contains your data
data_merged$day <- as.Date(data_merged$time, format = "%Y-%m-%d %H:%M:%S")

data_summarized <- data_merged %>%
  group_by(bldg_id, time, in.county) %>%
  summarize(
    total_energy_consumed = sum(total_energy_consumed, na.rm = TRUE),
    total_energy_produced = sum(out.electricity.pv.energy_consumption, na.rm = TRUE),
    
    total_Direct_Normal_Radiation = sum(`Direct Normal Radiation [W/m2]`, na.rm = TRUE),
    total_Diffuse_Horizontal_Radiation = sum(`Diffuse Horizontal Radiation [W/m2]`, na.rm = TRUE),
    median_Dry_Bulb_Temperature = median(`Dry Bulb Temperature [Â°C]`, na.rm = TRUE),
    median_Relative_Humidity = median(`Relative Humidity [%]`, na.rm = TRUE),
    median_Wind_Speed = median(`Wind Speed [m/s]`, na.rm = TRUE),
    median_Wind_Direction = median(`Wind Direction [Deg]`, na.rm = TRUE),
    total_Global_Horizontal_Radiation = sum(`Global Horizontal Radiation [W/m2]`, na.rm = TRUE),
    .groups = 'drop'  )

#summary(data_summarized)

```

```{r}
# Read the static house data
static_house_info = read_parquet("https://intro-datascience.s3.us-east-2.amazonaws.com/SC-data/static_house_info.parquet")

data_summarized <- merge(data_summarized, static_house_info, by = "bldg_id")
```


```{r}
# Save the data frame into a parquet file
write_parquet(data_summarized, "/Users/mj/Desktop/IDS_Nikitha/data_summarized.parquet")
```

####################################### Data Cleaning ###########################################

```{r}
data_summarized<- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_summarized.parquet")
data_cleaned <- data_summarized
```

```{r}
# Remove Columns with Zero Variance
data_cleaned <- data_cleaned[, sapply(data_cleaned, function(x) length(unique(x)) > 1)]
```

```{r}
# Convert the 'in.refrigerator' column to numeric
data_cleaned$in.refrigerator <- gsub("EF |, 100% Usage", "", data_cleaned$in.refrigerator)
data_cleaned$in.refrigerator[data_cleaned$in.refrigerator == "None"] <- "0"
data_cleaned$in.refrigerator <- as.numeric(data_cleaned$in.refrigerator)

```

```{r}
# Define a function to convert range to mean
range_to_mean <- function(range_str) {
  # Handle cases where the value is greater than a number (e.g., ">100000")
  if (grepl(">", range_str)) {
    # Assuming ">X" means "X+1" for the purposes of finding a mean
    return(as.numeric(gsub(">", "", range_str)) + 1)
  }
  
  # Handle cases where the value is less than a number (e.g., "<3000")
  if (grepl("<", range_str)) {
    # Assuming "<X" means "X-1" for the purposes of finding a mean
    return(as.numeric(gsub("<", "", range_str)) - 1)
  }

  # Split the string on the hyphen
  parts <- strsplit(range_str, "-")[[1]]
  
  # Remove any '+' signs and convert to numeric
  parts <- as.numeric(gsub("\\+", "", parts))
  
  # Calculate the mean of the two numbers
  if (length(parts) == 2) {
    return(mean(parts))
  } else {
    # If there's no range, just return the number itself
    return(parts[1])
  }
}

# Apply this function to each of the specified columns
data_cleaned$in.income <- sapply(data_cleaned$in.income, range_to_mean)
data_cleaned$in.income_recs_2015 <- sapply(data_cleaned$in.income_recs_2015, range_to_mean)
data_cleaned$in.income_recs_2020 <- sapply(data_cleaned$in.income_recs_2020, range_to_mean)

```


```{r}
# Remove 'Hour' prefix and convert to numeric
data_cleaned$in.bathroom_spot_vent_hour <- as.numeric(sub("Hour", "", data_cleaned$in.bathroom_spot_vent_hour))

# Now, the column 'in.bathroom_spot_vent_hour' should be numeric
data_cleaned$in.range_spot_vent_hour <- as.numeric(sub("Hour", "", data_cleaned$in.range_spot_vent_hour))

data_cleaned$in.cooling_setpoint <- as.numeric(sub("F", "", data_cleaned$in.cooling_setpoint))

data_cleaned$in.cooling_setpoint_offset_magnitude <- as.numeric(sub("F", "", data_cleaned$in.cooling_setpoint_offset_magnitude))

data_cleaned$in.heating_setpoint <- as.numeric(sub("F", "", data_cleaned$in.heating_setpoint))

data_cleaned$in.heating_setpoint_offset_magnitude <- as.numeric(sub("F", "", data_cleaned$in.heating_setpoint_offset_magnitude))

data_cleaned$in.infiltration <- as.numeric(sub(" ACH50", "", data_cleaned$in.infiltration))

data_cleaned$in.infiltration <- as.numeric(sub(" ACH50", "", data_cleaned$in.infiltration))
```

```{r}
# Remove 'Car' and convert the remaining part to numeric, replace 'None' with 0
data_cleaned$in.geometry_garage <- ifelse(data_cleaned$in.geometry_garage == "None", 0, as.numeric(gsub(" Car", "", data_cleaned$in.geometry_garage)))

```

```{r}
# Create a new column 'energy_usage_group' based on these thresholds
low_threshold <- 0.726
medium_threshold <- 1.059
high_threshold <- 1.559

data_cleaned$energy_usage_group <- dplyr::case_when(
  data_cleaned$total_energy_consumed < low_threshold ~ 'Low',
  data_cleaned$total_energy_consumed >= low_threshold & data_cleaned$total_energy_consumed < medium_threshold ~ 'Medium',
  data_cleaned$total_energy_consumed >= medium_threshold & data_cleaned$total_energy_consumed < high_threshold ~ 'High',
  data_cleaned$total_energy_consumed >= high_threshold ~ 'Very High',
  TRUE ~ 'Unknown'
)


data_cleaned$building_size <- cut(data_cleaned$in.sqft,
                                       breaks=c(min(data_cleaned$in.sqft), 1220, 2176, max(data_cleaned$in.sqft)),
                                       labels=c("Small", "Medium", "Large"),
                                       include.lowest=TRUE)
```

```{r}
# Impute missing values
data_cleaned <- data_cleaned %>%
  # Impute numeric columns with their mean
  mutate_if(is.numeric, ~ ifelse(is.na(.), mean(., na.rm = TRUE), .)) %>%
  # Impute categorical columns with the mode (most frequent value)
  mutate_if(is.character, function(x) {
    mode_value <- names(sort(table(x), decreasing = TRUE))[1]
    ifelse(is.na(x), mode_value, x)
  })
```

```{r}
# Calculate Percentage of Missing Data
missing_percentage <- data_cleaned %>% 
  summarise(across(where(is.character), ~sum(is.na(.))/n()*100))

#View(missing_percentage)
# no missing categorical data

#Check for non-standard missing values and replace them with NA
data_cleaned <- data_cleaned %>% 
  mutate(across(where(is.character), ~na_if(., ""))) # Replace empty strings with NA

# Calculate the percentage of missing data for each column
missing_percentage <- colSums(is.na(data_cleaned)) / nrow(data_cleaned) * 100

# Identify columns where more than 70% of data is missing
columns_to_remove <- names(missing_percentage[missing_percentage > 70])

# Remove these columns from the dataframe
data_cleaned <- data_cleaned[, !(names(data_cleaned) %in% columns_to_remove)]

# Function to calculate the mode
get_mode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}

# Replace NA in each column with its mode
data_cleaned <- data_cleaned %>%
  mutate(across(where(is.character), ~ifelse(is.na(.), get_mode(.), .)))

```

```{r}
# Save the data frame into a parquet file
write_parquet(data_cleaned, "/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```


####################################### Data Exploration ###########################################

```{r}
data_eda=read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```

```{r}
ggplot(data_eda, aes(x = in.weather_file_city, fill = in.heating_fuel)) + 
  geom_bar() + 
  labs(title = "Types of Fuels", x = "City") + 
  guides(color = guide_legend(title = "Fuel Type")) +
  theme(plot.title = element_text(hjust = 0.5),axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r}
ggplot(data_eda, aes(x = as.factor(in.geometry_stories), 
                      y = total_energy_consumed)) + 
  geom_boxplot() + 
  labs(title = "Energy Consumption in One and Two Story Buildings", 
       x = "Stories", y = "Energy Consumption") +
  theme(plot.title = element_text(hjust = 0.5))
```


```{r}
#Create a heatmap using ggplot2 to visualize the correlation matrix

# Calculate the correlation matrix
library(reshape2)
weather_columns <- c(
  
  'total_Diffuse_Horizontal_Radiation',
  'total_Direct_Normal_Radiation',
  'median_Dry_Bulb_Temperature',
  'median_Relative_Humidity',
  'median_Wind_Speed',
  'median_Wind_Direction',
  'total_Global_Horizontal_Radiation'
)

weather_and_energy_df <- data_eda[, c(weather_columns, 'total_energy_consumed')]
correlation_matrix <- cor(weather_and_energy_df)
ggplot(data = melt(correlation_matrix), aes(Var1, Var2, fill = value)) +
  geom_tile() +
  scale_fill_gradient(low = "red", high = "yellow") +
  theme_minimal() +
  labs(x = "Variables", y = "Variables", fill = "Correlation") +
  ggtitle("Correlation Matrix Between Weather Data and Total Energy Usage") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```



```{r}
library(dplyr)
library(ggplot2)

# Assuming 'in.county' is the column representing counties
average_temp_by_day <- data_eda %>%
  summarize(average_temperature = mean(median_Dry_Bulb_Temperature, na.rm = TRUE))

# Assuming 'total_energy_consumed' is the column representing daily total electricity usage
average_electricity_by_day <- data_eda %>%
  summarize(average_electricity = mean(total_energy_consumed, na.rm = TRUE))

# Plot with two axes
ggplot() +
  geom_line(data = average_temp_by_day, aes(x = time, y = average_temperature, color = "Temperature"), size = 1) +
  geom_line(data = average_electricity_by_day, aes(x = time, y = average_electricity, color = "Electricity"), size = 1) +
  scale_color_manual(values = c("Temperature" = "blue", "Electricity" = "red")) +
  labs(x = "Date", y = "Temperature (Â°C)", color = "Variable") +
  ggtitle("Average Temperature and Electricity Over Time (July 2018)") +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    legend.position = "bottom"
  ) +
  # Create a secondary y-axis for electricity
  scale_y_continuous(
    name = "Temperature (Â°C)",
    sec.axis = sec_axis(~ ., name = "Electricity (kWh)", 
                       labels = function(x) sprintf("%.0f", x))
  )


```

```{r}
ggplot(data_eda, aes(x = in.weather_file_city, 
                     y = (total_energy_consumed-total_energy_produced))) + 
  geom_bar(stat = "identity", fill = "blue") +
  geom_line(aes(y = total_energy_produced), color = "red", size = 1) +
  labs(title = "Energy Consumption and Production by City", 
       x = "City", y = "Total Energy") +
  theme(plot.title = element_text(hjust = 0.5),
        axis.text.x = element_text(angle = 45, hjust = 1))

# we can see that the enrgy consumption in Daniel Field, Maxton, and Rutherfordton are low as they are good producers of renewable energy
```


####################################### Data Modelling ###########################################

```{r}
library(caret)
library(arrow)
data_modelling <- read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
```

```{r}
#Assuming that negative values occur because the house is generating more energy than it consumes, meaning it doesn't require any energy from our company. Therefore, converting every negative value to zero.
data_modelling$total_energy_consumed[data_modelling$total_energy_consumed < 0] <- 0
summary(data_modelling$total_energy_consumed)
```

```{r}
data_modelling<- data_modelling[,-1]
data_modelling<- data_modelling[,-4]
```

```{r}
# Remove Columns with Zero Variance
data_modelling <- data_modelling[, sapply(data_modelling, function(x) length(unique(x)) > 2)]
```

```{r}
# Split the data into training and testing sets
set.seed(123)
trainIndex_1 <- createDataPartition(data_modelling$total_energy_consumed , p = 0.8, list = FALSE)
train_data_1 <- data_modelling[trainIndex_1, ]
test_data_1 <- data_modelling[-trainIndex_1, ]
```

```{r}

#now lets run a lm model with new dataframe 
lmout1 <- lm(total_energy_consumed ~ ., data = train_data_1)

# Print summary
summary(lmout1)
```

```{r}
# Predicted values from the linear regression model
predicted_values <- predict(lmout1, newdata = test_data_1)
# Create a logical index to filter negative and NA values
valid_index <- !is.na(predicted_values) & predicted_values >= 0


predicted_values <- predicted_values[valid_index]
# Actual values from the test dataset
actual_values <- test_data_1$total_energy_consumed[valid_index]

# Calculate MAE, MSE, RMSE, and R-squared
MAE <- mean(abs(predicted_values - actual_values))
MSE <- mean((predicted_values - actual_values)^2)
RMSE <- sqrt(mean((predicted_values - actual_values)^2))
R_squared <- 1 - (sum((actual_values - predicted_values)^2) / sum((actual_values - mean(actual_values))^2))

# Print the metrics
cat("Mean Absolute Error (MAE):", MAE, "\n")
cat("Mean Squared Error (MSE):", MSE, "\n")
cat("Root Mean Squared Error (RMSE):", RMSE, "\n")
cat("R-squared (RÂ²):", R_squared, "\n")

```


####################################### Data Prediction for 5 Degree warmer temperature ###########################################

```{r}
data_5_warmer <- data_modelling
data_5_warmer$median_Dry_Bulb_Temperature =data_5_warmer$median_Dry_Bulb_Temperature+5
predicted_energy_5_warmer = predict(lmout1, newdata = data_5_warmer)


```


```{r}
data_cleaned<-read_parquet("/Users/mj/Desktop/IDS_Nikitha/data_cleaned.parquet")
data_final<-data_cleaned
data_final$predicted_energy_5_warmer<-predicted_energy_5_warmer

# Remove rows where predicted_energy_5_warmer is 0 or NA
data_final <- na.omit(data_final[!(is.na(data_final$predicted_energy_5_warmer) | data_final$predicted_energy_5_warmer < 0), ])

summary(data_final$predicted_energy_5_warmer)
```
```{r}

```

```{r}
# Save the data frame into a parquet file
write_parquet(data_final, "/Users/mj/Desktop/IDS_Nikitha/data_final.parquet")
```
```{r}
library(tidyverse)
# Save the data frame into a parquet file
write_csv(data_final, "/Users/mj/Desktop/IDS_Nikitha/data_final.csv")
```














